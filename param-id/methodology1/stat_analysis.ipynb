{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from src.preprocessing.data_processing import process_file_from_wizard\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set the font size globally\n",
    "mpl.rcParams.update({'font.size': 13})  # Change 14 to your desired font size\n",
    "\n",
    "\n",
    "folder_path = \"data/exp15_02/*.csv\"\n",
    "folder_path = \"data/exp15_02/without_J_ext/*.csv\"\n",
    "# folder_path = \"data/exp15_02/with_J_ext/*.csv\"\n",
    "folder_path = \"data/exp15_02/without_J_ext_2/*.csv\"\n",
    "\n",
    "# List to store each processed dataframe\n",
    "processed_dataframes = []\n",
    "\n",
    "# Iterate over all CSV files in the folder\n",
    "for file_path in glob.glob(folder_path):\n",
    "    # Apply processing function to the dataframe\n",
    "    processed_df = process_file_from_wizard(file_path)\n",
    "\n",
    "    # processed_df = split_experiments(processed_df)\n",
    "    \n",
    "    # Store the processed dataframe in the list\n",
    "    processed_dataframes.append(processed_df)\n",
    "\n",
    "external_inertia = 0.0022421143208 # Msolo\n",
    "# external_inertia = 0\n",
    "Ra = 9.3756 # [Ohm]\n",
    "kt = 2.6657\n",
    "ke = 3.6103\n",
    "U_a = 12\n",
    "\n",
    "# (0.12834928717151478, 0.22012086337018552, 0.0730515032605435) on test 2\n",
    "tau_c = 0.12834928717151478\n",
    "c_v = 0.22012086337018552\n",
    "J = 0.0730515032605435 + external_inertia\n",
    "tau_c = 0.14628898896564263\n",
    "c_v = 0.20278725799569575\n",
    "J = 0.0720357539855984 + external_inertia\n",
    "len(processed_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices_of_start(df):\n",
    "    indices_of_change = []\n",
    "    for i in range(len(df) - 1):\n",
    "        if df['U'].iloc[i] == 0 and df['U'].iloc[i + 1] != 0:\n",
    "            indices_of_change.append(i)\n",
    "    return indices_of_change\n",
    "\n",
    "df_starting = []\n",
    "for df in processed_dataframes:\n",
    "    index = find_indices_of_start(df)[0]\n",
    "    df = df[index+4:index+80].reset_index(drop=True)\n",
    "    # df = df[(df['U'] == 0) & (df['U'].shift(1) != 0)][:].reset_index(drop=True)\n",
    "    plt.plot(df['t']-df['t'].iloc[0], abs(df['DXL_Velocity']))\n",
    "    df_starting.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices_of_end(df):\n",
    "    indices_of_change = []\n",
    "    for i in range(len(df) - 1):\n",
    "        if df['U'].iloc[i] == 0 and df['U'].iloc[i - 1] != 0:\n",
    "            indices_of_change.append(i)\n",
    "    return indices_of_change\n",
    "\n",
    "df_ending = []\n",
    "for df in processed_dataframes:\n",
    "    index = find_indices_of_end(df)[0]\n",
    "    df = df[index+15:index+290].reset_index(drop=True)\n",
    "    # df = df[(df['U'] == 0) & (df['U'].shift(1) != 0)][:].reset_index(drop=True)\n",
    "    plt.plot(df['t']-df['t'].iloc[0], abs(df['DXL_Velocity']))\n",
    "    df_ending.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_f = []\n",
    "for df in processed_dataframes:\n",
    "    df = df[:]\n",
    "    # df = df[df['U'] != 0][:100].reset_index(drop=True)\n",
    "    # plt.plot(df['t']-df['t'].iloc[0], abs(df['DXL_Velocity']))\n",
    "    df = df[df['U'] != 0].reset_index(drop=True)\n",
    "\n",
    "    plt.plot(abs(df['DXL_Velocity'][80:]))\n",
    "    v_f.append(abs(df['DXL_Velocity'][80:].mean()))\n",
    "\n",
    "import statistics\n",
    "statistics.mean(v_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motor_dynamics_updated(q_dot, t, tau_c, c_v, J):\n",
    "    if q_dot <= 0:\n",
    "        return 0\n",
    "    else :\n",
    "        return (-tau_c - c_v * q_dot) / J\n",
    "\n",
    "# Initialize arrays for all experimental data\n",
    "all_t = np.array([])\n",
    "all_velocity = np.array([])\n",
    "\n",
    "# Placeholder for plotting the simulated data along with the experimental data\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Process each dataframe in the list and simulate the corresponding response\n",
    "for count, df in enumerate(df_ending):\n",
    "    # Initial velocity for the current experiment\n",
    "    q_dot_0 = v_f[count]\n",
    "    \n",
    "    # Extract time and velocity from the current experiment\n",
    "    t_exp = df['t'] - df['t'].iloc[0]\n",
    "    velocity_exp = abs(df['DXL_Velocity'].reset_index(drop=True))\n",
    "    \n",
    "    # Combine the data from this experiment with the overall data\n",
    "    all_t = np.concatenate((all_t, t_exp))\n",
    "    all_velocity = np.concatenate((all_velocity, velocity_exp))\n",
    "    \n",
    "    # Plot the experimental data\n",
    "    ax.plot(t_exp, velocity_exp, label=f'Experiment {count+1}')\n",
    "    \n",
    "    # Simulate the deceleration phase for the current experiment\n",
    "    t_sim = np.linspace(0, t_exp.iloc[-1], len(t_exp))\n",
    "    q_dot_simulated = odeint(motor_dynamics_updated, q_dot_0, t_sim, args=(tau_c, c_v, J)).flatten()\n",
    "    \n",
    "    # Plot the simulated data\n",
    "    ax.plot(t_sim, q_dot_simulated, label=f'Simulation {count+1}', linestyle='--')\n",
    "\n",
    "# Enhance the plot\n",
    "ax.set_title('Experimental vs Simulated Motor Dynamics')\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel(r'$\\dot{q}$ (Angular Velocity)')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder code for calculating and plotting the combined error distribution for all experiments\n",
    "\n",
    "# Initialize arrays to hold all errors\n",
    "all_errors = np.array([])\n",
    "\n",
    "# Process each dataframe in the list and calculate the error distribution\n",
    "for count, df in enumerate(df_ending):\n",
    "    # Initial velocity for the current experiment\n",
    "    q_dot_0 = v_f[count]\n",
    "    \n",
    "    # Extract time and velocity from the current experiment\n",
    "    t_exp = df['t'] - df['t'].iloc[0]\n",
    "    velocity_exp = abs(df['DXL_Velocity'].reset_index(drop=True))\n",
    "    \n",
    "    # Simulate the deceleration phase for the current experiment\n",
    "    t_sim = np.linspace(0, t_exp.iloc[-1], len(t_exp))\n",
    "    q_dot_simulated = odeint(motor_dynamics_updated, q_dot_0, t_sim, args=(tau_c, c_v, J)).flatten()\n",
    "    \n",
    "    # Calculate the error (experimental - simulated)\n",
    "    error = (velocity_exp - q_dot_simulated)\n",
    "    \n",
    "    # Combine the errors from this experiment with the overall error array\n",
    "    all_errors = np.concatenate((all_errors, error))\n",
    "\n",
    "# Plot the combined error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_errors, bins=100, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Combined Error Distribution for All Experiments')\n",
    "plt.xlabel('Error (Real - Predicted Velocity)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate histogram data\n",
    "counts, bin_edges = np.histogram(all_errors, bins=100, density=True)\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "# Convert counts to probabilities\n",
    "probabilities = counts / counts.sum()\n",
    "\n",
    "# Fit a normal distribution to the data\n",
    "mu, std = norm.fit(all_errors)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_edges[:-1], counts, width=bin_width, color='blue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Plot the PDF of the fitted normal distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "# Add title and labels to the plot\n",
    "# plt.title('Error Distribution for Experiments with external load with Normal Fit')\n",
    "plt.xlabel('Error (Real - Predicted Velocity)')\n",
    "plt.ylabel('Probability \\n Density', rotation=0, labelpad=40)\n",
    "\n",
    "# Show the fit parameters on the plot\n",
    "plt.text(xmin, max(counts), f'Fit Results: mu = {mu:.3f}, std = {std:.3f}', fontsize=12)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'all_errors' is an array containing the error data\n",
    "\n",
    "# Calculate histogram data\n",
    "counts, bin_edges = np.histogram(all_errors, bins=100, density=False)\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "# Convert counts to probabilities\n",
    "probabilities = counts / counts.sum()\n",
    "\n",
    "# Plot the histogram as probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_edges[:-1], probabilities, width=bin_width, color='blue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Combined Probability Distribution for All Experiments')\n",
    "plt.xlabel('Error (Real - Predicted Velocity)')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a figure for the error distribution plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Process each dataframe in the list and calculate the error distribution\n",
    "for count, df in enumerate(df_ending):\n",
    "    # Initial velocity for the current experiment\n",
    "    q_dot_0 = v_f[count]\n",
    "    \n",
    "    # Extract time and velocity from the current experiment\n",
    "    t_exp = df['t'] - df['t'].iloc[0]\n",
    "    velocity_exp = abs(df['DXL_Velocity'].reset_index(drop=True))\n",
    "    \n",
    "    # Simulate the deceleration phase for the current experiment\n",
    "    t_sim = np.linspace(0, t_exp.iloc[-1], len(t_exp))\n",
    "    q_dot_simulated = odeint(motor_dynamics_updated, q_dot_0, t_sim, args=(tau_c, c_v, J)).flatten()\n",
    "    \n",
    "    # Calculate the error (experimental - simulated)\n",
    "    error = velocity_exp - q_dot_simulated\n",
    "    \n",
    "    # Plot the error distribution for the current experiment\n",
    "    ax.hist(error, bins=30, alpha=0.5, label=f'Experiment {count+1}')\n",
    "\n",
    "# Enhance the plot\n",
    "ax.set_title('Error Distribution for All Experiments')\n",
    "ax.set_xlabel('Error (Real - Predicted Velocity)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
